{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "import transformers\n",
    "from transformers import GPT2Tokenizer\n",
    "import backoff\n",
    "from openai.error import RateLimitError\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA generation using semantic search of the full textbook using GPT-3 embeddings and completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as infile:\n",
    "        return infile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key=openai.api_key=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt3_embedding(content, model='text-similarity-ada-001'):\n",
    "    try:\n",
    "        response = openai.Embedding.create(input=content, engine=model)\n",
    "    except openai.error.APIConnectionError:\n",
    "        print(\"Failed\") \n",
    "    return response['data'][0]['embedding'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute cosine similarity\n",
    "def get_similarity(v1, v2):\n",
    "    cosine = np.dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching through textbook \n",
    "def search_index(query, data, count=1):\n",
    "    question_vector = gpt3_embedding(query)\n",
    "    scores = []\n",
    "    for i in data:\n",
    "        score = get_similarity(question_vector, i['vector'])\n",
    "        scores.append({'content' : i['content'], 'score' : score})\n",
    "    most_relevant= sorted(scores, key=lambda d: d['score'], reverse=True)\n",
    "    return most_relevant[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(backoff.expo, RateLimitError)\n",
    "def response_API(prompt, myKwargs = {}):\n",
    "\n",
    "  #default arguments to send the API, unless changed in function\n",
    "  kwargs = {\"model\" :\"text-davinci-002\",\n",
    "            \"temperature\" :0.46,\n",
    "            \"max_tokens\": 300,\n",
    "            \"best_of\" :5,\n",
    "            \"n\" :3,\n",
    "            \"frequency_penalty\":0,\n",
    "            \"presence_penalty\":0}\n",
    "\n",
    "\n",
    "  for kwarg in myKwargs:\n",
    "    kwargs[kwarg] = myKwargs[kwarg]\n",
    "\n",
    "  r = openai.Completion.create(prompt=prompt, **kwargs)\n",
    "  return r['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_completions_with_backoff(passages): \n",
    "       \n",
    "    question_prompts = ['''Generate 5 interactive and coherent questions about this context. The questions should not be repeated from the previous step. The questions should consist of reasoning and procedural steps. \\n\n",
    "                        The questions should be precise and factual. Start the question with a '[Q]' ''',\n",
    "                        \n",
    "                        '''Generate 5 objective, concise and firm questions about this context. The questions should not be repeated from the previous step. \\n\n",
    "                        The questions should begin with any of Why/How/Where/Who/When. Start the question with a '[Q]' ''' ,\n",
    "                        \n",
    "                        '''Generate 5 thoughtful and compelling, steps-based procedural questions about this context that start with Why or How. The questions should not be repeated from the previous step. \\n\n",
    "                        The questions should be unique and creative with an abstract and subjective aspect. Start the question with a '[Q]' ''' ]\n",
    "    \n",
    "    n=len(question_prompts)\n",
    "    questions = []\n",
    "    for p in passages:\n",
    "        for j in question_prompts:\n",
    "                #prompt_tokens = calculate_tokens(j)\n",
    "                #context_tokens = calculate_tokens(p)\n",
    "                #max_tokens = 300\n",
    "                \n",
    "                #while(max_tokens+prompt_tokens+context_tokens < 4096):\n",
    "                prompt= \"%s \\n %s\" % (j, p)\n",
    "    \n",
    "                response = response_API(prompt)\n",
    "                \n",
    "                questions.append(response)\n",
    "                print(response)\n",
    "                      \n",
    "    question_list = [questions[i:i + n] for i in range(0, len(questions), n)]\n",
    "    \n",
    "    return question_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question, data):\n",
    "    #most relevant passages\n",
    "    result = search_index(question, data) #get most relevant passages where answer could be\n",
    "    prompt = \"PASSAGE - %s \\n QUESTION - %s \\nAnswer this question in 2-3 concise sentences based on the passage. Be objective in the answer given and explain in a few lines only.\\n\" % (result['content'], question)\n",
    "    answer = response_API(prompt)\n",
    "    print(answer)\n",
    "    \n",
    "    return answer, result['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sections data\n",
    "s = open(\"GPT-3_section_level.json\")\n",
    "sections_data = json.load(s)\n",
    "\n",
    "#full textbook embeddings - vectors\n",
    "with open(\"index.json\") as input_file:\n",
    "    data = json.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list of only texts from the json file\n",
    "sections_list = []\n",
    "for p, item in enumerate(sections_data):\n",
    "    subtext = item['positive_ctxs']['text']\n",
    "    sections_list.append(subtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating questions \n",
    "question_list = question_completions_with_backoff(sections_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting questions\n",
    "subq = []\n",
    "for i in question_list:\n",
    "    q = str(i).split('[Q]')\n",
    "    subq.append(q)\n",
    "    \n",
    "questions_1d = []\n",
    "\n",
    "for row in range(len(subq)):\n",
    "    for col in range(len(subq[row])):\n",
    "        questions_1d.append(subq[row][col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions cleaning - \n",
    "\n",
    "def clean_questions(questions_1d):\n",
    "\n",
    "    questions_cleaned = []\n",
    "    questions_cleaned = [item for item in questions_1d if item!= \"\"\"['\\\\n\"\"\" and item!= \"\"\"['\"\"\" and item!= \"\"\"['\\\\n\\\\n\"\"\"\n",
    "                        and item!=\"\"\"[\"\\\\n\"\"\" and item!=\"\"\"['\\\\n\\\\nQ: \"\"\" and item!= \"\"\"[\"\\\\n\\\\n\"\"\"\n",
    "                        and item!= \"\"\"['Q: \"\"\"]\n",
    "\n",
    "    ques = []\n",
    "    for i in questions_cleaned:\n",
    "        q = i.split('Q:')\n",
    "        ques.append(q)\n",
    "\n",
    "    questions_fin = []\n",
    "\n",
    "    for row in range(len(ques)):\n",
    "        for col in range(len(ques[row])):\n",
    "            questions_fin.append(ques[row][col])\n",
    "\n",
    "    questions = [item for item in questions_fin if item!= \"\"\"['\\\\n\"\"\" and item!= \"\"\"['\"\"\" and item!= \"\"\"['\\\\n\\\\n\"\"\"\n",
    "                        and item!=\"\"\"[\"\\\\n\"\"\" and item!=\"\"\"['\\\\n\\\\nQ: \"\"\" and item!= \"\"\"[\"\\\\n\\\\n\"\"\"\n",
    "                        and item!= \"\"\"['Q: \"\"\" and item!= \"\"\"['\\\\n\"\"\"]\n",
    "    \n",
    "    return questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = clean_questions(questions_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get answers using semantic search from textbook vector \n",
    "relevant_context = []\n",
    "answers = []\n",
    "for q in questions[0:650]:\n",
    "    external_answer, context = get_answer(q, data)\n",
    "    answers.append(external_answer)\n",
    "    relevant_context.append(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting the data\n",
    "qa_data = []\n",
    "for i, answer in enumerate(answers):\n",
    "    data = {}\n",
    "    data['textbook-paragraph'] = relevant_context[i]\n",
    "    data['GPT-3-Semantic-Search-Generations'] = {}\n",
    "    data['GPT-3-Semantic-Search-Generations']['question'] = questions[i]\n",
    "    data['GPT-3-Semantic-Search-Generations']['answer'] = answer    \n",
    "    qa_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('GPT-3_semantic_search.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(qa_data, f, ensure_ascii=False, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
